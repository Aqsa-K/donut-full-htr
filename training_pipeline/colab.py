# -*- coding: utf-8 -*-
"""Fine_tune_Donut_on_1880.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HT7Qz1E68ziDbr-X9DjdKQszvlSKURuU

## Set-up environment

First, let's install the relevant libraries:
* ðŸ¤— Transformers, for the model
* ðŸ¤— Datasets, for loading + processing the data
* PyTorch Lightning, for training the model
* Weights and Biases, for logging metrics during training
* Sentencepiece, used for tokenization.

We'll use PyTorch Lightning for training here, but note that this is optional, you can of course also just train in native PyTorch or use ðŸ¤— Accelerate, or the ðŸ¤— Trainer.
"""

# !pip install -q transformers datasets sentencepiece

# !pip install -q pytorch-lightning wandb

# !pip install -U datasets

"""## Load dataset

Next, let's load the dataset from the [hub](https://huggingface.co/datasets/naver-clova-ix/cord-v2). The dataset consists of (image, JSON) pairs. Note that it doesn't have to be JSON, it could also be JSON lines, plain text, etc.
"""

import os
from huggingface_hub import login

hf_token = os.getenv("HF_TOKEN")  # e.g., set via Colab secret or manually
login(token=hf_token)


from huggingface_hub import whoami

user_info = whoami()
print("Logged in as:", user_info["name"])


import os
import wandb

wandb_api_key = os.getenv("WANDB_API_KEY")
wandb.login(key=wandb_api_key)

user = wandb.api.viewer()
print(f"Logged in as: {user.get('entity')}")



dataset_name_hf = "AqsaK/testdataHW2"

from datasets import load_dataset

# dataset = load_dataset("AqsaK/1880_census_handwritten_archives")
# dataset = load_dataset("naver-clova-ix/cord-v2")
# dataset = load_dataset(dataset_name_hf, split='train[:5%]')
dataset = load_dataset(dataset_name_hf)

dataset

"""Let's take a look at the first training example:"""

example = dataset['train'][0]
image = example['image']
# let's make the image a bit smaller when visualizing
width, height = image.size
# display(image)

print(width, height)

# let's load the corresponding JSON dictionary (as string representation)
ground_truth = example['ground_truth']
print(ground_truth)

"""We can also parse the string as a Python dictionary using `ast.literal_eval`. Each training example has a single "gt_parse" key, which contains the ground truth parsing of the document:"""

from ast import literal_eval

literal_eval(ground_truth)['gt_parse']

import json
name_tokens = set()
# get all names and add them as tokens
for item in dataset["train"]:
    sample = json.loads(item["ground_truth"]) #["gt_parse"]
    print(sample)
    people = sample["gt_parse"]["households"]["individuals"]

    for person in people:
      if "FN" in person.keys():
        for token in person["FN"].split():
            name_tokens.add(token)
      if "EN" in person.keys():
        for token in person["EN"].split():
          name_tokens.add(token)
    # break
# Add new tokens
# name_tokens = ["Carl", "Gustaf", "Sven", "Axel", "Clara", "Edvard", "Ludvig", "Gustavva"]
# added = processor.tokenizer.add_tokens(name_tokens)

# # Resize the model's embeddings
# model.decoder.resize_token_embeddings(len(processor.tokenizer))

len(name_tokens)

max_length = 0
for item in dataset["train"]:
    sample = item["ground_truth"] #["gt_parse"]
    print(sample)
    est_length = int(len(str(sample)))
    if est_length > max_length:
        max_length = est_length
    break

max_length

# max_length = 14811

max_width = 0
max_height = 0
for item in dataset["train"]:
    image = item["image"] #["gt_parse"]
    width, height = image.size
    # print(width, height)
    if width > max_width:
        max_width = width
    if height > max_height:
        max_height = height

print(max_width, max_height)

"""## Load model and processor

Next, we load the model (Donut is an instance of [VisionEncoderDecoderModel](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder)), and the processor, which is the object that can be used to prepare inputs for the model.

We'll update some settings for fine-tuning, namely the image size and the max length of the decoder.
"""

from transformers import VisionEncoderDecoderConfig

# image_size = [1280, 960] #for naver clova dataset
image_size = [max_width, max_height] # for census data
max_length = max_length #14811

# update image_size of the encoder
# during pre-training, a larger image size was used
config = VisionEncoderDecoderConfig.from_pretrained("naver-clova-ix/donut-base")
config.encoder.image_size = image_size # (height, width)
# update max_length of the decoder (for generation)
config.decoder.max_length = max_length
# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:
# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602

"""Next, we instantiate the model with our custom config, as well as the processor. Make sure that all pre-trained weights are correctly loaded (a warning would tell you if that's not the case)."""

1000/1.27

from transformers import DonutProcessor, VisionEncoderDecoderModel

processor = DonutProcessor.from_pretrained("naver-clova-ix/donut-base")
model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base", config=config)

processor.feature_extractor.size = [1000, 800]

# Add new tokens
# added = processor.tokenizer.add_tokens(list(name_tokens))

# # Resize the model's embeddings
# model.decoder.resize_token_embeddings(len(processor.tokenizer))

# processor.feature_extractor.do_resize = True
# processor.feature_extractor.size = 1600
# processor.feature_extractor.resample = <interpolation_method>
# processor.feature_extractor.do_align_long_axis = True

"""## Create PyTorch dataset

Here we create a regular PyTorch dataset.

The model doesn't directly take the (image, JSON) pairs as input and labels. Rather, we create `pixel_values` and `labels`. Both are PyTorch tensors. The `pixel_values` are the input images (resized, padded and normalized), and the `labels` are the `input_ids` of the target sequence (which is a flattened version of the JSON), with padding tokens replaced by -100 (to make sure these are ignored by the loss function). Both are created using `DonutProcessor` (which internally combines an image processor, for the image modality, and a tokenizer, for the text modality).

Note that we're also adding tokens to the vocabulary of the decoder (and corresponding tokenizer) for all keys of the dictionaries in our dataset, like "\<s_menu>". This makes sure the model learns an embedding vector for them. Without doing this, some keys might get split up into multiple subword tokens, in which case the model just learns an embedding for the subword tokens, rather than a direct embedding for these keys.
"""

import json
import random
from typing import Any, List, Tuple

import torch
from torch.utils.data import Dataset

from PIL import Image, UnidentifiedImageError

from PIL import Image
import io

added_tokens = []

class DonutDataset(Dataset):
    """
    PyTorch Dataset for Donut. This class takes a HuggingFace Dataset as input.

    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),
    and it will be converted into pixel_values (vectorized image) and labels (input_ids of the tokenized string).

    Args:
        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl
        max_length: the max number of tokens for the target sequences
        split: whether to load "train", "validation" or "test" split
        ignore_id: ignore_index for torch.nn.CrossEntropyLoss
        task_start_token: the special token to be fed to the decoder to conduct the target task
        prompt_end_token: the special token at the end of the sequences
        sort_json_key: whether or not to sort the JSON keys
    """

    def __init__(
        self,
        dataset_name_or_path: str,
        max_length: int,
        split: str = "train",
        ignore_id: int = -100,
        task_start_token: str = "<s>",
        prompt_end_token: str = None,
        sort_json_key: bool = True,
    ):
        super().__init__()

        self.max_length = max_length
        self.split = split
        self.ignore_id = ignore_id
        self.task_start_token = task_start_token
        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token
        self.sort_json_key = sort_json_key

        self.dataset = load_dataset(dataset_name_or_path, split=self.split)
        self.dataset_length = len(self.dataset)

        print(self.dataset_length)

        k= 0

        self.gt_token_sequences = []
        self.valid_indices = []  # Store valid indices

        for i in range(self.dataset_length):
            try:
              sample = self.dataset[i]
              # print(k)
              # print(sample)
              ground_truth = json.loads(sample["ground_truth"])
              if "gt_parses" in ground_truth:  # when multiple ground truths are available, e.g., docvqa
                  assert isinstance(ground_truth["gt_parses"], list)
                  gt_jsons = ground_truth["gt_parses"]
                  # print("gt parses added successfully")
              else:
                  assert "gt_parse" in ground_truth and isinstance(ground_truth["gt_parse"], dict)
                  gt_jsons = [ground_truth["gt_parse"]]
                  # print("gt parse added successfully")


              self.gt_token_sequences.append(
                  [
                      self.json2token(
                          gt_json,
                          update_special_tokens_for_json_key=self.split == "train",
                          sort_json_key=self.sort_json_key,
                      )
                      + processor.tokenizer.eos_token
                      for gt_json in gt_jsons  # load json from list of json
                  ]
              )
              self.valid_indices.append(i) # Add valid index
              # break
            except Exception as e:
              print(f"Skipping bad image: {e}")
              k+=1
              continue
            k+=1
            # if k > 500:
              # break

        self.add_tokens([self.task_start_token, self.prompt_end_token])
        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)

    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):
        """
        Convert an ordered JSON object into a token sequence
        """
        if type(obj) == dict:
            # print("inside json2token if")
            if len(obj) == 1 and "text_sequence" in obj:
                return obj["text_sequence"]
            else:
                output = ""
                if sort_json_key:
                    keys = sorted(obj.keys(), reverse=True)
                else:
                    keys = obj.keys()
                for k in keys:
                    # print("key: ", k)
                    if update_special_tokens_for_json_key:
                        # print("adding token")
                        self.add_tokens([fr"<s_{k}>", fr"</s_{k}>"])
                    output += (
                        fr"<s_{k}>"
                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)
                        + fr"</s_{k}>"
                    )
                # print("output: ", output)
                return output
        elif type(obj) == list:
            return r"<sep/>".join(
                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]
            )
        else:
            obj = str(obj)
            if f"<{obj}/>" in added_tokens:
                obj = f"<{obj}/>"  # for categorical special tokens
            return obj

    def add_tokens(self, list_of_tokens: List[str]):
        """
        Add special tokens to tokenizer and resize the token embeddings of the decoder
        """
        # print("inside add token function: ", list_of_tokens)
        special_tokens_dict = {"additional_special_tokens": list_of_tokens}
        newly_added_num = processor.tokenizer.add_special_tokens(special_tokens_dict)
        # print("newly_added_num: ", newly_added_num)

        if newly_added_num > 0:
            # print("token added to token list")
            # print("added tokens: ", list_of_tokens)
            model.decoder.resize_token_embeddings(len(processor.tokenizer))
            added_tokens.extend(list_of_tokens)

    def __len__(self) -> int:
        return len(self.valid_indices)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Load image from image_path of given dataset_path and convert into input_tensor and labels
        Convert gt data into input_ids (tokenized string)
        Returns:
            input_tensor : preprocessed image
            input_ids : tokenized gt_data
            labels : masked labels (model doesn't need to predict prompt and pad token)
        """
        # Use the valid index to access the original dataset
        true_idx = self.valid_indices[idx]
        sample = self.dataset[true_idx]
        # sample = self.dataset[idx]

        # # inputs
        # # Check if image data is valid before attempting to open:
        # image_bytes = sample["image"].tobytes()
        # if not image_bytes:  # or len(image_bytes) < 100:  # Add a size check if needed
        #     raise ValueError(f"Invalid image data at index {idx}")

        # # Open image using Pillow with format hint (if available):
        # try:
        #     image = Image.open(io.BytesIO(image_bytes)).convert("RGB")  # Explicitly convert to RGB
        # except UnidentifiedImageError:
        #     # Handle the error, e.g., skip the sample, replace with a placeholder image, etc.
        #     print(f"WARNING: Could not open image at index {idx}, skipping...")
        #     return None  # Or handle differently
        # pixel_values = processor(image, random_padding=self.split == "train", return_tensors="pt").pixel_values
        # pixel_values = pixel_values.squeeze()

        # inputs
        pixel_values = processor(sample["image"].convert("RGB"), random_padding=self.split == "train", return_tensors="pt").pixel_values
        pixel_values = pixel_values.squeeze()

        # targets
        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1
        full_sequence = self.task_start_token + target_sequence

        input_ids = processor.tokenizer(
            full_sequence,
            add_special_tokens=False,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )["input_ids"].squeeze(0)

        labels = input_ids.clone()
        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token
        # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)
        return pixel_values, labels, full_sequence

"""Next, we instantiate the datasets:"""

# we update some settings which differ from pretraining; namely the size of the images + no rotation required
# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml
processor.image_processor.size = image_size[::-1] # should be (width, height)
processor.image_processor.do_align_long_axis = False

train_dataset = DonutDataset(dataset_name_hf, max_length=max_length,
                             split="train", task_start_token="<s_arch-v1>", prompt_end_token="<s_arch-v1>",
                             sort_json_key=False, # cord dataset is preprocessed, so no need for this
                             )

# val_dataset = DonutDataset("naver-clova-ix/cord-v2", max_length=max_length,
#                              split="validation", task_start_token="<s_cord-v2>", prompt_end_token="<s_cord-v2>",
#                              sort_json_key=False, # cord dataset is preprocessed, so no need for this
#                              )


val_dataset = DonutDataset(dataset_name_hf, max_length=max_length,
                             split="validation", task_start_token="<s_arch-v1>", prompt_end_token="<s_arch-v1>",
                             sort_json_key=False, # cord dataset is preprocessed, so no need for this
                             )



# train_dataset = DonutDataset("naver-clova-ix/cord-v2", max_length=max_length,
#                              split="train", task_start_token="<s_cord-v2>", prompt_end_token="<s_cord-v2>",
#                              sort_json_key=False, # cord dataset is preprocessed, so no need for this
#                              )

# val_dataset = DonutDataset("naver-clova-ix/cord-v2", max_length=max_length,
#                              split="validation", task_start_token="<s_cord-v2>", prompt_end_token="<s_cord-v2>",
#                              sort_json_key=False, # cord dataset is preprocessed, so no need for this
#                              )

from torch.utils.data import Subset
# Create a subset of the first 100 samples
val_dataset = Subset(val_dataset, indices=list(range(50)))

"""Let's check which tokens are added:"""

len(added_tokens)

print(added_tokens)

# the vocab size attribute stays constants (might be a bit unintuitive - but doesn't include special tokens)
print("Original number of tokens:", processor.tokenizer.vocab_size)
print("Number of tokens after adding special tokens:", len(processor.tokenizer))

sample = val_dataset[0]
image_tensor, labels_tensor, prompt_text = sample

print("Prompt text:", prompt_text)

assert prompt_text.startswith("<s_arch-v1>"), "Missing start token!"

input_ids = processor.tokenizer(prompt_text, add_special_tokens=False)["input_ids"]
decoded = processor.tokenizer.decode(input_ids, skip_special_tokens=False)

print("Decoded back:", decoded)

"""You can verify that a token like `</s_unitprice>` was added to the vocabulary of the tokenizer (and the model):"""

processor.decode([70])

"""As always, it's very important to verify whether our data is prepared correctly. Let's check the first training example:"""

train_dataset[0]

pixel_values, labels, target_sequence = train_dataset[0]

"""This returns the `pixel_values` (the image, but prepared for the model as a PyTorch tensor), the `labels` (which are the encoded `input_ids` of the target sequence, which we want Donut to learn to generate) and the original `target_sequence`. The reason we also return the latter is because this will allow us to compute metrics between the generated sequences and the ground truth target sequences."""

print(pixel_values.shape)

# let's print the labels (the first 30 token ID's)
for id in labels.tolist()[:30]:
  if id != -100:
    print(processor.decode([id]))
  else:
    print(id)

# let's check the corresponding target sequence, as a string
print(target_sequence)

"""Another important thing is that we need to set 2 additional attributes in the configuration of the model. This is not required, but will allow us to train the model by only providing the decoder targets, without having to provide any decoder inputs.

The model will automatically create the `decoder_input_ids` (the decoder inputs) based on the `labels`, by shifting them one position to the right and prepending the decoder_start_token_id. I recommend checking [this video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&t=888s&ab_channel=NielsRogge) if you want to understand how models like Donut automatically create decoder_input_ids - and more broadly how Donut works.
"""

model.config.pad_token_id = processor.tokenizer.pad_token_id
# model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s_cord-v2>'])[0]
model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s_arch-v1>'])[0]

processor.decode(model.config.decoder_start_token_id)

# sanity check
print("Pad token ID:", processor.decode([model.config.pad_token_id]))
print("Decoder start token ID:", processor.decode([model.config.decoder_start_token_id]))

"""## Create PyTorch DataLoaders

Next, we create corresponding PyTorch DataLoaders, which allow us to loop over the dataset in batches:
"""

from torch.utils.data import DataLoader

# feel free to increase the batch size if you have a lot of memory
# I'm fine-tuning on Colab and given the large image size, batch size > 1 is not feasible
train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)
val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)

"""Let's verify a batch:"""

batch = next(iter(train_dataloader))
pixel_values, labels, target_sequences = batch
print(pixel_values.shape)

for id in labels.squeeze().tolist()[:30]:
  if id != -100:
    print(processor.decode([id]))
  else:
    print(id)

print(len(train_dataset))
print(len(val_dataset))

# let's check the first validation batch
batch = next(iter(val_dataloader))
pixel_values, labels, target_sequences = batch
print(pixel_values.shape)

print(target_sequences[0])

"""## Define LightningModule

Next, we define a [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html), which is the standard way to train a model in PyTorch Lightning. A LightningModule is an `nn.Module` with some additional functionality.

Basically, PyTorch Lightning will take care of all device placements (`.to(device)`) for us, as well as the backward pass, putting the model in training mode, etc.
"""

from pathlib import Path
import re
from nltk import edit_distance
import numpy as np
import math

from torch.nn.utils.rnn import pad_sequence
from torch.optim.lr_scheduler import LambdaLR

import pytorch_lightning as pl
from pytorch_lightning.utilities import rank_zero_only


class DonutModelPLModule(pl.LightningModule):
    def __init__(self, config, processor, model):
        super().__init__()
        self.config = config
        self.processor = processor
        self.model = model

    def training_step(self, batch, batch_idx):
        pixel_values, labels, _ = batch

        outputs = self.model(pixel_values, labels=labels)
        loss = outputs.loss
        self.log("train_loss", loss)
        return loss

    # def training_step(self, batch, batch_idx):
    #   # Forward pass
    #   outputs = self.model(pixel_values, labels=labels)
    #   logits = outputs.logits  # (B, T, V)
    #   labels = labels

    #   # Shift for CLM
    #   shift_logits = logits[..., :-1, :].contiguous()
    #   shift_labels = labels[..., 1:].contiguous()

    #   # Decode labels to strings
    #   flat_labels = shift_labels.view(-1)
    #   token_strs = self.processor.tokenizer.batch_decode(flat_labels, skip_special_tokens=False)

    #   # Define known tag tokens to exclude from value boosting
    #   known_tags = {
    #       "<s>", "</s>", "<sep/>"
    #   }.union({f"<{t}>" for t in self.processor.tokenizer.additional_special_tokens}) \
    #   .union({f"</{t[1:]}" for t in self.processor.tokenizer.additional_special_tokens if t.startswith("<s_")})

    #   # Create weights: 2.0 for values, 1.0 for tags
    #   weights = torch.tensor([
    #       2.0 if tok not in known_tags else 1.0
    #       for tok in token_strs
    #   ], device=self.device).view(shift_labels.shape)

    #   # Standard cross-entropy loss
    #   loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction="none")
    #   loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

    #   # Weighted average loss
    #   weighted_loss = (loss * weights.view(-1)).sum() / weights.ne(-100).sum()
    #   self.log("train_loss", weighted_loss)

    #   print("computed weighted loss")

    #   return weighted_loss


    def validation_step(self, batch, batch_idx, dataset_idx=0):
        pixel_values, labels, answers = batch

        # print("pixel_values:", pixel_values)
        # print("labels:", labels)
        # print("answers: ", answers)

        batch_size = pixel_values.shape[0]
        # we feed the prompt to the model
        decoder_input_ids = torch.full((batch_size, 1), self.model.config.decoder_start_token_id, device=self.device)

        outputs = self.model.generate(pixel_values,
                                   decoder_input_ids=decoder_input_ids,
                                   max_length=max_length,
                                   early_stopping=True,
                                   pad_token_id=self.processor.tokenizer.pad_token_id,
                                   eos_token_id=self.processor.tokenizer.eos_token_id,
                                   use_cache=True,
                                   num_beams=1,
                                  #  temperature=0.6,
                                  #  top_k=50,
                                  #  top_p=0.95,
                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],
                                   return_dict_in_generate=True,)

        predictions = []
        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):
            seq = seq.replace(self.processor.tokenizer.eos_token, "").replace(self.processor.tokenizer.pad_token, "")
            seq = re.sub(r"<.*?>", "", seq, count=1).strip()  # remove first task start token
            predictions.append(seq)

        scores = []
        for pred, answer in zip(predictions, answers):
            pred = re.sub(r"(?:(?<=>) | (?=</s_))", "", pred)
            # NOT NEEDED ANYMORE
            # answer = re.sub(r"<.*?>", "", answer, count=1)
            answer = answer.replace(self.processor.tokenizer.eos_token, "")
            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))

            if self.config.get("verbose", False) and len(scores) == 1:
                print(f"Prediction: {pred}")
                print(f"    Answer: {answer}")
                print(f" Normed ED: {scores[0]}")

        self.log("val_edit_distance", np.mean(scores))

        return scores

    def configure_optimizers(self):
        # you could also add a learning rate scheduler if you want
        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get("lr"))

        return optimizer

    def train_dataloader(self):
        return train_dataloader

    def val_dataloader(self):
        return val_dataloader

"""## Train!

Next, let's train! This happens instantiating a PyTorch Lightning `Trainer`, and then calling `trainer.fit`.

What's great is that we can automatically train on the hardware we have (in our case, a single GPU), enable mixed precision (`fp16=True`, which makes sure we don't consume as much memory), add Weights and Biases logging, and so on.
"""

config = {"max_epochs":10,
          "val_check_interval":0.2, # how many times we want to validate during an epoch
          "check_val_every_n_epoch":1,
          "gradient_clip_val":1.0,
          "num_training_samples_per_epoch": 1200,
          "lr":3e-5,
          "train_batch_sizes": [8],
          "val_batch_sizes": [1],
          # "seed":2022,
          "num_nodes": 1,
          "warmup_steps": 300, # 800/8*30/10, 10%
          "result_path": "./result",
          "verbose": True,
          "seed": 42
          }

model_module = DonutModelPLModule(config, processor, model)

"""We'll use a custom callback to push our model to the hub during training (after each epoch + end of training). For that we'll log into our HuggingFace account."""

# !git config --global credential.helper store

# !huggingface-cli login

from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.callbacks import Callback, EarlyStopping

wandb_logger = WandbLogger(project="Donut", name="demo-run-1880")

class PushToHubCallback(Callback):
    def on_train_epoch_end(self, trainer, pl_module):
        print(f"Pushing model to the hub, epoch {trainer.current_epoch}")
        pl_module.model.push_to_hub("AqsaK/donut_cord",
                                    commit_message=f"Training in progress, epoch {trainer.current_epoch}")

    def on_train_end(self, trainer, pl_module):
        print(f"Pushing model to the hub after training")
        pl_module.processor.push_to_hub("AqsaK/donut_cord",
                                    commit_message=f"Training done")
        pl_module.model.push_to_hub("AqsaK/donut_cord",
                                    commit_message=f"Training done")

# early_stop_callback = EarlyStopping(monitor="val_edit_distance", patience=3, verbose=False, mode="min")

trainer = pl.Trainer(
        accelerator="cpu",
        devices=1,
        max_epochs=config.get("max_epochs"),
        val_check_interval=config.get("val_check_interval"),
        check_val_every_n_epoch=config.get("check_val_every_n_epoch"),
        gradient_clip_val=config.get("gradient_clip_val"),
        precision=16, # we'll use mixed precision
        num_sanity_val_steps=0,
        logger=wandb_logger,
        callbacks=[PushToHubCallback()],
)

trainer.fit(model_module)

"""## Evaluate

After training, we can evaluate the model on the test set.

As we pushed the model to the hub, we can very easily load it back again using the `from_pretrained` method. You can see in the [repo](https://huggingface.co/nielsr/donut-demo/tree/main) that we have the following files:

Note that you can also easily refer to a specific commit in the `from_pretrained` method using the [`revision`](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained.revision) argument, or use the private hub in case you'd like to keep your models private and only shared with certain colleagues for instance.

Here we're just loading from the main branch, which means the latest commit.
"""

from transformers import DonutProcessor, VisionEncoderDecoderModel

processor = DonutProcessor.from_pretrained("AqsaK/donut_cord")
model = VisionEncoderDecoderModel.from_pretrained("AqsaK/donut_cord")

"""As we don't have a test split here, let's evaluate on the validation split.

We'll use the `token2json` method of the processor to turn the generated sequences into JSON, and the `JSONParseEvaluator` object available in the Donut package.
"""

# !pip install -q donut-python

import re
import json
import torch
from tqdm.auto import tqdm
import numpy as np

from donut import JSONParseEvaluator

from datasets import load_dataset

device = "cuda" if torch.cuda.is_available() else "cpu"

model.eval()
model.to(device)

output_list = []
accs = []

dataset = load_dataset("AqsaK/1880_census_handwritten_archives", split="train")

for idx in tqdm(range(len(dataset))):

# for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):
  try:
      # prepare encoder inputs
      pixel_values = processor(sample["image"].convert("RGB"), return_tensors="pt").pixel_values
      pixel_values = pixel_values.to(device)
      # prepare decoder inputs
      task_prompt = "<s_cord-v2>"
      decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors="pt").input_ids
      decoder_input_ids = decoder_input_ids.to(device)

      # autoregressively generate sequence
      outputs = model.generate(
              pixel_values,
              decoder_input_ids=decoder_input_ids,
              max_length=model.decoder.config.max_position_embeddings,
              early_stopping=True,
              pad_token_id=processor.tokenizer.pad_token_id,
              eos_token_id=processor.tokenizer.eos_token_id,
              use_cache=True,
              num_beams=1,
              bad_words_ids=[[processor.tokenizer.unk_token_id]],
              return_dict_in_generate=True,
          )

      # turn into JSON
      seq = processor.batch_decode(outputs.sequences)[0]
      seq = seq.replace(processor.tokenizer.eos_token, "").replace(processor.tokenizer.pad_token, "")
      seq = re.sub(r"<.*?>", "", seq, count=1).strip()  # remove first task start token
      seq = processor.token2json(seq)

      ground_truth = json.loads(sample["ground_truth"])
      ground_truth = ground_truth["gt_parse"]
      evaluator = JSONParseEvaluator()
      score = evaluator.cal_acc(seq, ground_truth)

      accs.append(score)
      output_list.append(seq)
  except Exception as e:
      print("Exception: ", e)
      continue

scores = {"accuracies": accs, "mean_accuracy": np.mean(accs)}
print(scores, f"length : {len(accs)}")

print("Mean accuracy:", np.mean(accs))

